import os
import requests
from typing import List, Dict, Optional
from .vector_store import VectorStore

class RAGPipeline:
    """RAG pipeline using Hugging Face Inference API with DeepSeek model"""
    
    def __init__(self, hf_token: Optional[str] = None):
        self.hf_token = hf_token or os.environ.get('HF_TOKEN')
        self.vector_store = VectorStore()
        
        # Updated to use the working DeepSeek model
        self.model_url = "https://router.huggingface.co/hf-inference/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B/v1/chat/completions"   
        self.model_name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"

        # self.model_url = "https://router.huggingface.co/hf-inference/models/mistralai/Mixtral-8x7B-Instruct-v0.1/v1/chat/completions"   
        # self.model_name = "mistralai/Mixtral-8x7B-Instruct-v0.1"
    
    def add_documents(self, texts: List[str], metadata: List[Dict] = None):
        """Add documents to the vector store"""
        self.vector_store.add_documents(texts, metadata)
    
    def generate_response(self, prompt: str, model_url: str = None, model_name: str = None) -> str:
        """Generate response using Hugging Face Chat Completions API"""
        if not self.hf_token:
            return "Error: HF_TOKEN not found. Please set your Hugging Face token."
        
        url = model_url or self.model_url
        model = model_name or self.model_name
        
        headers = {
            "Authorization": f"Bearer {self.hf_token}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": model,
            "messages": [
                {
                    "role": "system",
                    "content": "You are a helpful assistant that answers questions based on provided context. Be concise and accurate."
                },
                {
                    "role": "user", 
                    "content": prompt
                }
            ],
            "max_tokens": 150,
            "temperature": 0.3,
            "stream": False
        }
        
        try:
            response = requests.post(url, headers=headers, json=payload, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                
                # Extract response from chat completions format
                if "choices" in result and len(result["choices"]) > 0:
                    message = result["choices"][0].get("message", {})
                    return message.get("content", "").strip()
                else:
                    return "No response generated from the model."
                    
            elif response.status_code == 429:
                return "Rate limit exceeded. Please try again later."
            elif response.status_code == 401:
                return "Authentication failed. Please check your HF_TOKEN."
            elif response.status_code == 422:
                # Handle deserialization errors
                error_message = response.json().get("error", "Unknown error")
                return f"Deserialization error: {error_message}"
            else:
                return f"Error: Unable to generate response (Status: {response.status_code})"
        
        except requests.exceptions.Timeout:
            return "Request timeout. Please try again."
        except requests.exceptions.ConnectionError:
            return "Connection error. Please check your internet connection."
        except Exception as e:
            return f"Error generating response: {str(e)}"
    
    def generate_response_with_fallback(self, prompt: str) -> str:
        """Generate response with fallback to backup models"""
        # Try primary model first
        response = self.generate_response(prompt)
        
        # If primary model fails, try backup models
        if response.startswith("Error") or response.startswith("Rate limit") or response.startswith("Authentication"):
            print(f"Primary model failed: {response}")
            
            for backup_name, backup_config in self.backup_models.items():
                print(f"Trying backup model: {backup_name}")
                backup_response = self.generate_response(
                    prompt, 
                    backup_config["url"], 
                    backup_config["name"]
                )
                
                if not backup_response.startswith("Error"):
                    return f"{backup_response} (Generated by {backup_name} model)"
            
            # If all models fail, return fallback message
            return "Unable to generate response from any model. Please try again later or check your token."
        
        return response
    
    def create_context_prompt(self, query: str, contexts: List[str]) -> str:
        """Create a prompt with context for the LLM"""
        # Limit context length to prevent token limits
        max_context_length = 800
        limited_contexts = []
        
        for i, ctx in enumerate(contexts):
            if len(ctx) > max_context_length // len(contexts):
                ctx = ctx[:max_context_length // len(contexts)] + "..."
            limited_contexts.append(f"Context {i+1}: {ctx}")
        
        context_text = "\n\n".join(limited_contexts)
        
        prompt = f"""Answer the question based only on the provided context information. Be concise and accurate.

        Context Information:
        {context_text}

        Question: {query}

        Instructions:
        - Only use information from the provided context
        - If the context doesn't contain relevant information, say so
        - Keep your answer concise and to the point
        - Do not make up information not present in the context

        Answer:"""
        
        return prompt
    
    def query(self, question: str, k: int = 3) -> Dict:
        """Query the RAG system"""
        # Retrieve relevant documents
        similar_docs = self.vector_store.similarity_search(question, k=k)
        
        if not similar_docs:
            return {
                "answer": "I don't have enough information to answer this question. Please upload some documents first.",
                "sources": [],
                "confidence": 0.0,
                "source_count": 0
            }
        
        # Extract contexts and calculate average confidence
        contexts = [doc[0] for doc in similar_docs]
        confidence_scores = [doc[1] for doc in similar_docs]
        avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0.0
        
        # Create prompt and generate answer
        prompt = self.create_context_prompt(question, contexts)
        answer = self.generate_response_with_fallback(prompt)
        
        return {
            "answer": answer,
            "sources": contexts,
            "confidence": avg_confidence,
            "source_count": len(similar_docs)
        }
    
        
    def test_connection(self) -> Dict:
        """Test the connection to the Hugging Face API"""
        test_prompt = "Hello, can you confirm you're working?"
        
        try:
            response = self.generate_response(test_prompt)
            
            if response.startswith("Error"):
                return {
                    "status": "failed",
                    "message": response
                }
            else:
                return {
                    "status": "success", 
                    "message": "Model is working correctly",
                    "response": response
                }
        except Exception as e:
            return {
                "status": "failed",
                "message": f"Connection test failed: {str(e)}"
            }